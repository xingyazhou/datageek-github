{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PySpark Basic Tutotial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.window import Window\n",
    "import pyspark.sql.functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = SparkContext(\"local\", \"First App\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SparkContext Example firstapp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lines with a: 65, lines with b: 33\n"
     ]
    }
   ],
   "source": [
    "logFile = \"C:///Users/xingy/Spark/spark-3.0.0-preview2-bin-hadoop3.2/README.md\"\n",
    "logData = sc.textFile(logFile).cache()\n",
    "numAs = logData.filter(lambda s: 'a' in s).count()\n",
    "numBs = logData.filter(lambda s: 'b' in s).count()\n",
    "print (\"Lines with a: %i, lines with b: %i\" % (numAs, numBs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RDD Operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of elements in RDD -> 8\n"
     ]
    }
   ],
   "source": [
    "words = sc.parallelize (\n",
    "   [\"scala\", \n",
    "   \"java\", \n",
    "   \"hadoop\", \n",
    "   \"spark\", \n",
    "   \"akka\",\n",
    "   \"spark vs hadoop\", \n",
    "   \"pyspark\",\n",
    "   \"pyspark and spark\"]\n",
    ")\n",
    "counts = words.count()\n",
    "print (\"Number of elements in RDD -> %i\" % (counts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### collect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elements in RDD -> ['scala', 'java', 'hadoop', 'spark', 'akka', 'spark vs hadoop', 'pyspark', 'pyspark and spark']\n"
     ]
    }
   ],
   "source": [
    "words = sc.parallelize (\n",
    "   [\"scala\", \n",
    "   \"java\", \n",
    "   \"hadoop\", \n",
    "   \"spark\", \n",
    "   \"akka\",\n",
    "   \"spark vs hadoop\", \n",
    "   \"pyspark\",\n",
    "   \"pyspark and spark\"]\n",
    ")\n",
    "coll = words.collect()\n",
    "print (\"Elements in RDD -> %s\" % (coll))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitered RDD spark in -> ['spark', 'spark vs hadoop', 'pyspark', 'pyspark and spark']\n",
      "Fitered RDD spark not in -> ['scala', 'java', 'hadoop', 'akka']\n"
     ]
    }
   ],
   "source": [
    "words = sc.parallelize (\n",
    "   [\"scala\", \n",
    "   \"java\", \n",
    "   \"hadoop\", \n",
    "   \"spark\", \n",
    "   \"akka\",\n",
    "   \"spark vs hadoop\", \n",
    "   \"pyspark\",\n",
    "   \"pyspark and spark\"]\n",
    ")\n",
    "words_filter1 = words.filter(lambda x: 'spark' in x)\n",
    "words_filter2 = words.filter(lambda x: 'spark' not in x)\n",
    "filtered1 = words_filter1.collect()\n",
    "filtered2 = words_filter2.collect()\n",
    "print (\"Fitered RDD spark in -> %s\" % (filtered1) )\n",
    "print (\"Fitered RDD spark not in -> %s\" % (filtered2) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Key value pair -> [('scala', 1), ('java', 1), ('hadoop', 1), ('spark', 1), ('akka', 1), ('spark vs hadoop', 1), ('pyspark', 1), ('pyspark and spark', 1)]\n"
     ]
    }
   ],
   "source": [
    "words = sc.parallelize (\n",
    "   [\"scala\", \n",
    "   \"java\", \n",
    "   \"hadoop\", \n",
    "   \"spark\", \n",
    "   \"akka\",\n",
    "   \"spark vs hadoop\", \n",
    "   \"pyspark\",\n",
    "   \"pyspark and spark\"]\n",
    ")\n",
    "\n",
    "words_map = words.map(lambda x: (x, 1))\n",
    "mapping = words_map.collect()\n",
    "print (\"Key value pair -> %s\" % (mapping))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding all the elements -> 15\n"
     ]
    }
   ],
   "source": [
    "from operator import add\n",
    "\n",
    "nums = sc.parallelize([1, 2, 3, 4, 5])\n",
    "adding = nums.reduce(add)\n",
    "print (\"Adding all the elements -> %i\" % (adding))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Join RDD -> [('hadoop', (4, 5)), ('spark', (1, 2))]\n"
     ]
    }
   ],
   "source": [
    "x = sc.parallelize([(\"spark\", 1), (\"hadoop\", 4)])\n",
    "y = sc.parallelize([(\"spark\", 2), (\"hadoop\", 5), ('Beam', 18)])\n",
    "\n",
    "joined = x.join(y)\n",
    "final = joined.collect()\n",
    "print (\"Join RDD -> %s\" % (final))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words got chached > True\n"
     ]
    }
   ],
   "source": [
    "words = sc.parallelize (\n",
    "   [\"scala\", \n",
    "   \"java\", \n",
    "   \"hadoop\", \n",
    "   \"spark\", \n",
    "   \"akka\",\n",
    "   \"spark vs hadoop\", \n",
    "   \"pyspark\",\n",
    "   \"pyspark and spark\"]\n",
    ") \n",
    "words.cache() \n",
    "caching = words.persist().is_cached \n",
    "print (\"Words got chached > %s\" % (caching))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Broadcast & Accumulator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For parallel processing, Apache Spark uses shared variables.<br>\n",
    "\n",
    "Here are two types of shared variables supported by Apache Spark: *Broadcast*, *Accumulator* <br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Broadcast\n",
    "Broadcast variables are used to save the copy of data across all nodes. This variable is cached on all the machines and not sent on machines with tasks. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.broadcast.Broadcast'>\n",
      "Stored data -> ['scala', 'java', 'hadoop', 'spark', 'akka']\n",
      "Printing a particular element in RDD -> hadoop\n"
     ]
    }
   ],
   "source": [
    "words_new = sc.broadcast([\"scala\", \"java\", \"hadoop\", \"spark\", \"akka\"]) \n",
    "\n",
    "print(type(words_new))\n",
    "data = words_new.value \n",
    "print (\"Stored data -> %s\" % (data) )\n",
    "\n",
    "elem = words_new.value[2] \n",
    "print (\"Printing a particular element in RDD -> %s\" % (elem))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accumulator\n",
    "Accumulator variables are used for aggregating the information through associative and commutative operations. For example, you can use an accumulator for a sum operation or counters (in MapReduce)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.accumulators.Accumulator'>\n",
      "Accumulated value is -> 240\n"
     ]
    }
   ],
   "source": [
    "num = sc.accumulator(100) \n",
    "\n",
    "def f(x): \n",
    "   global num \n",
    "   num+=x \n",
    "\n",
    "rdd = sc.parallelize([20,30,40,50]) \n",
    "rdd.foreach(f) \n",
    "\n",
    "final = num.value \n",
    "print(type(num))\n",
    "print (\"Accumulated value is -> %i\" % (final))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SparkFiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Absolute Path -> C:\\Users\\xingy\\AppData\\Local\\Temp\\spark-de06aa81-444f-4690-9f72-5a029e68f74f\\userFiles-16b7041c-6828-44ba-827d-8eb06892d7b8\\README.md\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkFiles\n",
    "\n",
    "finddistance = \"C:///Users/xingy/Spark/spark-3.0.0-preview2-bin-hadoop3.2/README.md\"\n",
    "finddistancename = \"README.md\"\n",
    "\n",
    "sc.addFile(finddistance)\n",
    "abs_path = SparkFiles.get(finddistancename) \n",
    "print (\"Absolute Path -> %s\" % abs_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## StorageLevel\n",
    "DISK_ONLY = StorageLevel(True, False, False, False, 1)\n",
    "\n",
    "DISK_ONLY_2 = StorageLevel(True, False, False, False, 2)\n",
    "\n",
    "MEMORY_AND_DISK = StorageLevel(True, True, False, False, 1)\n",
    "\n",
    "MEMORY_AND_DISK_2 = StorageLevel(True, True, False, False, 2)\n",
    "\n",
    "MEMORY_AND_DISK_SER = StorageLevel(True, True, False, False, 1)\n",
    "\n",
    "MEMORY_AND_DISK_SER_2 = StorageLevel(True, True, False, False, 2)\n",
    "\n",
    "MEMORY_ONLY = StorageLevel(False, True, False, False, 1)\n",
    "\n",
    "MEMORY_ONLY_2 = StorageLevel(False, True, False, False, 2)\n",
    "\n",
    "MEMORY_ONLY_SER = StorageLevel(False, True, False, False, 1)\n",
    "\n",
    "MEMORY_ONLY_SER_2 = StorageLevel(False, True, False, False, 2)\n",
    "\n",
    "OFF_HEAP = StorageLevel(True, True, True, False, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Disk Memory Serialized 2x Replicated\n",
      "Disk Serialized 1x Replicated\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "\n",
    "rdd1 = sc.parallelize([1,2])\n",
    "rdd1.persist( pyspark.StorageLevel.MEMORY_AND_DISK_2 )\n",
    "rdd1.getStorageLevel()\n",
    "\n",
    "rdd2 = sc.parallelize(['a','b', 'c'] )                      \n",
    "rdd2.persist( pyspark.StorageLevel.DISK_ONLY)\n",
    "                       \n",
    "print(rdd1.getStorageLevel())\n",
    "print(rdd2.getStorageLevel())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recommendation\n",
    "The following example is of collaborative filtering using ALS ( Alternating Least Squares) algorithm to build the recommendation model and evaluate it on training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error = 1.0770581106910645e-05\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "from pyspark import SparkContext\n",
    "from pyspark.mllib.recommendation import ALS, MatrixFactorizationModel, Rating\n",
    "\n",
    "data = sc.textFile(\"test.data.txt\")\n",
    "ratings = data.map(lambda l: l.split(',')).map(lambda l: Rating(int(l[0]), int(l[1]), float(l[2])))\n",
    "   \n",
    "# Build the recommendation model using Alternating Least Squares\n",
    "rank = 10\n",
    "numIterations = 10\n",
    "model = ALS.train(ratings, rank, numIterations)\n",
    "   \n",
    "# Evaluate the model on training data\n",
    "testdata = ratings.map(lambda p: (p[0], p[1]))\n",
    "predictions = model.predictAll(testdata).map(lambda r: ((r[0], r[1]), r[2]))\n",
    "ratesAndPreds = ratings.map(lambda r: ((r[0], r[1]), r[2])).join(predictions)\n",
    "MSE = ratesAndPreds.map(lambda r: (r[1][0] - r[1][1])**2).mean()\n",
    "print(\"Mean Squared Error = \" + str(MSE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Serializer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 2, 4, 6, 8, 10, 12, 14, 16, 18]\n"
     ]
    }
   ],
   "source": [
    "sc.stop()\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.serializers import MarshalSerializer\n",
    "sc = SparkContext(\"local\", \"serialization app\", serializer = MarshalSerializer())\n",
    "print(sc.parallelize(list(range(1000))).map(lambda x: 2 * x).take(10))\n",
    "sc.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
